<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EasyTalking">
  <meta name="keywords" content="EasyTalking">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EasyTalking (AAAI #4987)</title>

  <style>
    td {
        padding: 10px;
        border: 8px solid rgb(255, 255, 255);
    }
    h3,h4,section {
      margin: 0;
      padding: 0;
    }
    section {
      margin-bottom: -20px; /* 减小间距 */
    }
    video {
      margin-top: -20px; /* 减小间距 */
      margin-bottom: -20px; /* 减小间距 */
    }
    section:last-child {
      margin-bottom: 0; /* 最后一个section不减小间距 */
    }
    .iframe-container {
    display: flex;
    justify-content: space-between;
  }
  .iframe-container-one {
    display: flex;
    justify-content: space-between;
  }
  .iframe-container-one-small {
    display: flex;
    justify-content: space-between;
  }
  .iframe-container iframe {
    width: 49%; /* Adjust width as needed */
    height: 300px; /* Adjust height as needed */
    border: none;
  }
  .iframe-container-one iframe {
    width: 100%; /* Adjust width as needed */
    height: 360px; /* Adjust height as needed */
    border: none;
  }
  .iframe-container-one-small iframe {
    width: 100%; /* Adjust width as needed */
    height: 220px; /* Adjust height as needed */
    border: none;
  }
  .iframe-container-four iframe {
    width: 24%; /* Adjust width as needed */
    height: 240px; /* Adjust height as needed */
    border: none;
  }
  .italic {
  font-style: italic;
}
</style>

<!--   Global site tag (gtag.js) - Google Analytics-->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y5ZVQZ7NHC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-Y5ZVQZ7NHC');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

<link rel="apple-touch-icon-precomposed" sizes="120x120" href="./img/github-6980894_1280.png">
<link rel="icon" href="./img/github-6980894_1280.png" type="image/x-icon"/>
<link rel="shortcut icon" href="./img/github-6980894_1280.png" type="image/x-icon"/>

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>


</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EasyTalking: Cross-Modal Emotion Alignment for Emotional Talking Head Generation</h1>

          <div class="is-size-3 publication-authors">
            <span class="author-block"> </span>
            <span class="author-block"> 
              <h2 class="subtitle has-text-centered" style="font-size: 2.4rem; color: #29d4d2">
              AAAI 2025 Anonymous Submission #4987 </span>
            </h2>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!--teaser-->
<section class="hero teaser is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!--
      <h2 class="subtitle has-text-centered" style="font-size: 1.38rem; color: #1c1c1c">
        We present <span style="color: #2fd3fc"><b>EasyTalking</b></span>,
        <p>
        a cross-modal style controllable emotional talking head generation method.
      </h2>
      -->

      <h2 class="subtitle has-text-centered" style="font-size: 1.38rem; color: #1c1c1c">
        <span class="italic" style="color: #cd3925" ><b>(AAAI Rebuttal: We have uploaded some new results.)</b></span>
        <p>
      </h2>
    </div>
  </div>
</section>

<!--Abstract-->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Speech-driven talking head generation aims at synthesizing realistic talking video according to a speech, where notable advancements have been achieved. However, expressing designated emotion in one-shot scenario is still challenging due to the confusion between expressions and personal appearance and the heterogeneity of modalities of the emotion prompts. To increases the effectiveness and accessibility of emotion control, we propose the <b><u>EasyTalking</u></b>, a two-stage generation framework which allows emotion prompts in modalities of image, text, or speech. We construct the EasyTalking from two main intuitions: first, to decouple the learning of facial dynamics from tones and appearances for more effective emotion control; second, to learn a modal-agnostic emotion space for allowing emotion prompts in diverse modalities. Specifically, we leverage phonemes as toneless inputs and project the emotion prompts into a modal-agnostic emotion space, which are used to guide the proposed Talking Diffusion Transformer to generate the appearance-agnostic motion representations. After that, we adapt the PIRender model with the fine-grained facial loss to render video frames from the motion representations. Through such way, we ensure the facial expression is controlled by the emotion prompt while achieve new state-of-the-art measured by lip-sync and realistic metrics.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video Intro</h2>
        <h2 class="subtitle has-text-centered">
          (If the video is slightly lagging behind the audio, please try using a different device.) </h2>
          <div style="position:relative; width:100%; height:0px; padding-bottom:56.250%"><iframe allow="fullscreen" allowfullscreen height="100%" src="https://streamable.com/e/6egv2p?loop=0" width="100%" style="border:none; width:100%; height:100%; position:absolute; left:0px; top:0px; overflow:hidden;"></iframe></div>
      </div>
    </div>
  </div>
</section>


<!-- Methodology -->
<section class="hero is-light is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <!--/ Methodology. -->
        <div class="section-title">
          <h2 class="title is-3 is-centered">Methodology</h2>
        </div>
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <div class="publication-img">
              <img id="architecture" src="./assets/img/main.png"/>
            </div>
          </div>
        </div>
        <p>
          We divide the generation into the Speech-to-Motion and Motion-to-Video stages. In the first stage, we propose a Talking Diffusion Transformer to predict representations of holistic facial and head motion from phonemes and emotion prompts. Then we propse a render model to render video frames according to the predicted motion representations and the identity-reference image.
        </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

      <div class="section-title">
        <h2 class="title is-3">Showcases</h2>
      </div>
    <h3 class="title is-4">Audio-Driven (with expressions)</h3>
    <div id="results-carousel" class="carousel results-carousel">
      <div class="item item-compare1">
        <div class="section-title">
          <h4 class="title is-5">Emotion: Anger</h5>
        </div>
        <div class="iframe-container-one-small">
          <iframe allow="fullscreen" allowfullscreen height="426" src="https://streamable.com/e/ut8m90?loop=0" width="1278" style="border:none;"></iframe>
        </div>
      </div>
      <div class="item item-compare2">
        <div class="section-title">
          <h4 class="title is-5">Emotion: Surprised</h5>
        </div>
        <div class="iframe-container-one-small">
          <iframe allow="fullscreen" allowfullscreen height="426" src="https://streamable.com/e/ywd2wq?loop=0" width="1278" style="border:none;"></iframe>
        </div>
      </div>
    </div>

    <p>&nbsp;&nbsp;&nbsp;</p>
    <h3 class="title is-4">More cases<span class="italic" style="color: #ea3017" ><b> (New!)</b></span></h3>
    We demonstrate our generalizability by providing in-the-wild speech samples and conducting experiments on a Chinese dataset, CHERMA. Some cases are shown below.
    <div class="iframe-container">
      <iframe allow="fullscreen" allowfullscreen height="1080" src="https://streamable.com/e/047ew7?loop=0" width="1920" style="border:none;"></iframe>
      <iframe allow="fullscreen" allowfullscreen height="1080" src="https://streamable.com/e/fht9m5?loop=0" width="1920" style="border:none;"></iframe>
    </div>

    <p>&nbsp;&nbsp;&nbsp;</p>
    <h3 class="title is-4">Multi-modal Style Control</h3>
    <div class="iframe-container">
      <iframe allow="fullscreen" allowfullscreen height="1080" src="https://streamable.com/e/yd9d14?loop=0" width="1920" style="border:none;"></iframe>
      <iframe allow="fullscreen" allowfullscreen height="1080" src="https://streamable.com/e/xxyx0x?loop=0" width="1920" style="border:none;"></iframe>
    </div>

    <p>&nbsp;&nbsp;&nbsp;</p>
    <h3 class="title is-4">Multi-modal Alignment Results<span class="italic" style="color: #ea3017" ><b> (New!)</b></span></h3>
    Here are examples generated with prompts of different modalities from the same source.
    <div class="iframe-container-one">
      <iframe allow="fullscreen" allowfullscreen height="1080" src="https://streamable.com/e/qywgoh?loop=0" width="1920" style="border:none;"></iframe>
    </div>

    <p>&nbsp;&nbsp;&nbsp;</p>
    <h3 class="title is-4">Fine-grained Emotion Control</h3>
    For fine-grained emotion control, we adapt the classifier-free guidance to interpolate and extrapolate emotions. We can easily control the intensity of arbitrary emotion as follows.
    <p>&nbsp;&nbsp;&nbsp;</p>
    <h4 class="title is-5">Angry-Surprise</h4>
    <div id="results-carousel" class="carousel results-carousel">
      <div class="item item-compare1">
        <div class="iframe-container-one">
          <iframe allow="fullscreen" allowfullscreen height="720" src="https://streamable.com/e/wa7lss?loop=0" width="1280" style="border:none;"></iframe>
        </div>
      </div>
      <div class="item item-compare2">
        <div class="iframe-container-one">
          <iframe allow="fullscreen" allowfullscreen height="720" src="https://streamable.com/e/p7md5j?loop=0" width="1280" style="border:none;"></iframe>
        </div>
      </div>
    </div>

    <p>&nbsp;&nbsp;&nbsp;</p>
    <h4 class="title is-5">Neutral-Happy</h4>
    <div id="results-carousel" class="carousel results-carousel">
      <div class="item item-compare1">
        <div class="iframe-container-one">
          <iframe allow="fullscreen" allowfullscreen height="720" src="https://streamable.com/e/ppr0lm?loop=0" width="1280" style="border:none;"></iframe>
        </div>
      </div>
      <div class="item item-compare2">
        <div class="iframe-container-one">
          <iframe allow="fullscreen" allowfullscreen height="720" src="https://streamable.com/e/m4do51?loop=0" width="1280" style="border:none;"></iframe>
        </div>
      </div>
    </div>

    <p>&nbsp;&nbsp;&nbsp;</p>
    <h4 class="title is-5">Sad-Happy</h4>
    <div id="results-carousel" class="carousel results-carousel">
      <div class="item item-compare1">
        <div class="iframe-container-one">
          <iframe allow="fullscreen" allowfullscreen height="720" src="https://streamable.com/e/hm9qj8?loop=0" width="1280" style="border:none;"></iframe>
        </div>
      </div>
      <div class="item item-compare2">
        <div class="iframe-container-one">
          <iframe allow="fullscreen" allowfullscreen height="720" src="https://streamable.com/e/j6hhut?loop=0" width="1280" style="border:none;"></iframe>
        </div>
      </div>
    </div>

    <p>&nbsp;&nbsp;&nbsp;</p>
    <h3 class="title is-4">Different Expression Source Image Driven Results<span class="italic" style="color: #ea3017" ><b> (New!)</b></span></h3>
    Our model can generate varied expressions within the same emotion class through different expression style prompts.
    <div class="iframe-container-one">
      <iframe allow="fullscreen" allowfullscreen height="1080" src="https://streamable.com/e/41lnd7?loop=0" width="1920" style="border:none;"></iframe>
    </div>

  </div>
</section>



<!-- Baseline compare -->
<section class="section">
  <div class="container is-max-desktop">

      <div class="section-title">
        <h2 class="title is-3">Comparison With Current Methods</h2>
      </div>

    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <div class="columns is-vcentered interpolation-panel">
            <div class="column has-text-centered">
              <img src="./assets/img/sota.png"  width="55%">
              <p>Audio-driven talking head video generation methods to be compared.</p>
            </div>
          </div>
        </div>
      </div>

    </div>
    <h3 class="title is-4">Qualitative Evaluation</h3>
    <h4 class="title is-5">Part1. In-Domain Comparison</h4>
    <div id="results-carousel" class="carousel results-carousel">
      <div class="item item-compare1">
        <div class="iframe-container-one">
          <iframe allow="fullscreen" allowfullscreen height="720" src="https://streamable.com/e/x5mt7x?" width="1280" style="border:none;"></iframe>
        </div>
      </div>
      <div class="item item-compare2">
        <div class="iframe-container-one">
          <iframe allow="fullscreen" allowfullscreen height="720" src="https://streamable.com/e/hlkmor?loop=0" width="1280" style="border:none;"></iframe>
        </div>
      </div>
    </div>
    Our method follows the specified emotion and keeps the shape of face, while other methods fail in generating right lips, following the emotion, or keeping the shape of face.
    

    <p>&nbsp;&nbsp;&nbsp;</p>

    <h4 class="title is-5">Part2. In-The-Wild Comparison</h4>
    <div id="results-carousel" class="carousel results-carousel">
      <div class="item item-compare1">
        <div class="iframe-container-one">
          <iframe allow="fullscreen" allowfullscreen height="720" src="https://streamable.com/e/twl0ze?loop=0" width="1280" style="border:none;"></iframe>
        </div>
      </div>
      <div class="item item-compare2">
        <div class="iframe-container-one">
          <iframe allow="fullscreen" allowfullscreen height="720" src="https://streamable.com/e/wz7lwc?loop=0" width="1280" style="border:none;"></iframe>
        </div>
      </div>
    </div>
      Our method generates the most accurate lip when following the specific emotion.

    <br><br>
    <h3 class="title is-4">Quantitative Evaluation</h3>
    <div class="columns is-vcentered interpolation-panel">
      <div class="column has-text-centered">
        <img src="./assets/img/Quantitative_Evaluation.png">
        <p> Comparison with SOTAs on the MEAD dataset. The best results are bold and the second-best results are bracketed.</p>
      </div>
    </div>

    <br><br>
    <h3 class="title is-4">User Study</h3>
    <div class="columns is-vcentered interpolation-panel">
      <div class="column has-text-centered">
        <img src="./assets/img/user_study.png" width="55%">
        <p>
We conduct a user study with 30 users. We randomly select 10 speeches from the testing set: 5 for same-ID generation and the other 5 for cross-ID generation. Detailed setting is described in our supplement. We ask the users to separately identify the best anonymous models for each speech, according to lip synchronization, expression reality, and visual quality.</p>
      </div>
    </div>

  </div>
</section>

<!-- Ablation Studies -->
<section class="section">
  <div class="container is-max-desktop">

      <div class="section-title">
        <h2 class="title is-3">Ablation Studies</h2>
      </div>
    <div class="columns is-vcentered interpolation-panel">
      <div class="column has-text-centered">
        <img src="./assets/img/ablation.png" width="55%">
        <p>
          We start from using Wav2Vec2 features from speeches as the baseline. After using phoneme as input, the Sync and metrics of visual quality are improved, which indicates the model can learn better lip dynamics. Our deliberated causal mask further enhances the learning of phonemes. Our cross-modal emotion alignment learning significantly improves the emotion, which additionally improves the visual quality and lip sync. Finally, the proposed fine-grained facial loss further improves the expression generation and identity preservation by focusing on the eyes and mouth.</p>
      </div>
    </div>

    <h3 class="title is-4">Visualization Results</h3>
    <div id="results-carousel" class="carousel results-carousel">
      <div class="item item-compare1">
        <div class="iframe-container-one">
          <iframe allow="fullscreen" allowfullscreen height="720" src="https://streamable.com/e/1q5842?loop=0" width="1280" style="border:none;"></iframe>
        </div>
      </div>
      <div class="item item-compare2">
        <div class="iframe-container-one">
          <iframe allow="fullscreen" allowfullscreen height="720" src="https://streamable.com/e/8fikrh?loop=0" width="1280" style="border:none;"></iframe>
        </div>
      </div>
    </div>
    The model trained with Wav2Vec2.0 features tends to over-fit the training set, thus it performs worse in in-the-wild speeches and obstructs emotion control. The models taking as input phonemes produce better lips for in-the-wild speeches. However, they do not follow the text emotion prompt until the cross-modal emotion alignment is imposed. The fine-grained facial loss further improves the texture of the eyes and mouth.

  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
      <h1 class="title is-4">
          Ethical Consideration
      </h1>
      <div class="content has-text-justified-desktop">
          <p>

            We should use technology responsibly and be careful about the synthesized content. There is a risk that EasyTalking could be misused. Thus, we will restrict the access to our code and pre-trained models when making the code public, likely by validating the institutional email. Furthermore, we also consider embedding visible or invisible digital watermarks in any generated content.

          </p>

      </div>

      <h1 class="title is-4">
          Removal Policy
      </h1>
      <div class="content has-text-justified-desktop">

           <p>

         Please be aware that all videos on this page are algorithmically generated from publicly available sources and are intended solely for academic demonstrations and algorithm comparisons.  Any other form of usage is prohibited. Besides, if required by the original image owner or in the case of misuse of the models, the images, models, and codes associated with this project may be removed at any time.
          </p>
      </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website source code is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
