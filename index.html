<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EasyTalking">
  <meta name="keywords" content="EasyTalking">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EasyTalking (AAAI #4987)</title>

  <style>
    td {
        padding: 10px;
        border: 8px solid rgb(255, 255, 255);
    }
    h3,h4,section {
      margin: 0;
      padding: 0;
    }
    section {
      margin-bottom: -20px; /* 减小间距 */
    }
    video {
      margin-top: -20px; /* 减小间距 */
      margin-bottom: -20px; /* 减小间距 */
    }
    section:last-child {
      margin-bottom: 0; /* 最后一个section不减小间距 */
    }
</style>

<!--   Global site tag (gtag.js) - Google Analytics-->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y5ZVQZ7NHC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-Y5ZVQZ7NHC');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

<link rel="apple-touch-icon-precomposed" sizes="120x120" href="./img/github-6980894_1280.png">
<link rel="icon" href="./img/github-6980894_1280.png" type="image/x-icon"/>
<link rel="shortcut icon" href="./img/github-6980894_1280.png" type="image/x-icon"/>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>



</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EasyTalking: Cross-Modal Emotion Alignment for Emotional Talking Head Generation</h1>

          <div class="is-size-3 publication-authors">
            <span class="author-block"> </span>
            <span class="author-block"> 
              <h2 class="subtitle has-text-centered" style="font-size: 2.4rem; color: #29d4d2">
              AAAI 2025 Anonymous Submission #4987 </span>
            </h2>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!--teaser-->
<section class="hero teaser is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered" style="font-size: 1.38rem; color: #1c1c1c">
        We present <span style="color: #2fd3fc"><b>EasyTalking</b></span>,
        <p>
        a cross-modal style controllable emotional talking head generation method.
      </h2>
    </div>
  </div>
</section>

<!--Abstract-->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Speech-driven talking head generation aims at synthesizing realistic talking video according to a speech, where notable advancements have been achieved. However, expressing designated emotion in one-shot scenario is still challenging due to the confusion between expressions and personal appearance and the heterogeneity of modalities of the emotion prompts. To increases the effectiveness and accessibility of emotion control, we propose the <b><u>EasyTalking</u></b>, a two-stage generation framework which allows emotion prompts in modalities of image, text, or speech. We construct the EasyTalking from two main intuitions: first, to decouple the learning of facial dynamics from tones and appearances for more effective emotion control; second, to learn a modal-agnostic emotion space for allowing emotion prompts in diverse modalities. Specifically, we leverage phonemes as toneless inputs and project the emotion prompts into a modal-agnostic emotion space, which are used to guide the proposed Talking Diffusion Transformer to generate the appearance-agnostic motion representations. After that, we adapt the PIRender model with the fine-grained facial loss to render video frames from the motion representations. Through such way, we ensure the facial expression is controlled by the emotion prompt while achieve new state-of-the-art measured by lip-sync and realistic metrics.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!--Main Video-->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video Intro</h2>
        <h2 class="subtitle has-text-centered">
          (If the video is slightly lagging behind the audio, please try using a different device.) </h2>
        <video id="main_video" controls="" playsinline="" height="100%">
        <source src="./assets/video/demo.mp4" type="video/mp4">
        </video>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!-- Methodology -->
<section class="hero is-light is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <!--/ Methodology. -->
        <div class="section-title">
          <h2 class="title is-3 is-centered">Methodology</h2>
        </div>
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <div class="publication-img">
              <img id="architecture" src="./assets/img/main.png"/>
            </div>
          </div>
        </div>
        <p>
          We divide the generation into the Speech-to-Motion and Motion-to-Video stages. In the first stage, we propose a Talking Diffusion Transformer to predict representations of holistic facial and head motion from phonemes and emotion prompts. Then we propse a render model to render video frames according to the predicted motion representations and the identity-reference image.
        </p>
    </div>
  </div>
</section>

<!--
<section class="section">
  <div class="container is-max-desktop">

      <div class="section-title">
        <h2 class="title is-3">Showcases</h2>
      </div>

    <h3 class="title is-4">Audio-Driven (without expressions)</h3>
    <p>
      Audio driven cases without multi-modal style control.
    </p>
    <table style="border-collapse:separate; border-spacing:0px 10px;">
    <tr>
      <td width="50%"> <video id="showcases1" controls="" playsinline="" height="90%">
        <source src="./assets/video/test.mp4" type="video/mp4">
      </video> </td>
      <td width="50%"> <video id="showcases1" controls="" playsinline="" height="90%">
        <source src="./assets/video/test.mp4" type="video/mp4">
      </video> </td>
    </tr>
    </table>

    <br><br>
    <h3 class="title is-4">Multi-modal Style Control</h3>
    <table style="border-collapse:separate; border-spacing:0px 10px;">
    <tr>
      <td width="50%"> <video id="showcases1" controls="" playsinline="" height="90%">
        <source src="./assets/video/test.mp4" type="video/mp4">
      </video> </td>
      <td width="50%"> <video id="showcases1" controls="" playsinline="" height="90%">
        <source src="./assets/video/test.mp4" type="video/mp4">
      </video> </td>
    </tr>
    </table>

  </div>
</section>
-->

<!-- Baseline compare -->
<section class="section">
  <div class="container is-max-desktop">

      <div class="section-title">
        <h2 class="title is-3">Comparison With Current Methods</h2>
      </div>

    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <div class="columns is-vcentered interpolation-panel">
            <div class="column has-text-centered">
              <img src="./assets/img/sota.png"  width="55%">
              <p>Audio-driven talking head video generation methods to be compared.</p>
            </div>
          </div>
        </div>
      </div>

    </div>
    <h3 class="title is-4">Qualitative Evaluation</h3>
    <h4 class="title is-5">Part1. In-Domain Comparison</h4>
    <table style="border-collapse:separate; border-spacing:0px 10px;">
      <tr>
        <td width="50%"> <video id="showcases1" controls="" playsinline="" height="90%">
          <source src="./assets/video/in-domain case1 all.mp4" type="video/mp4">
        </video> </td>
        <td width="50%"> <video id="showcases2" controls="" playsinline="" height="90%">
          <source src="./assets/video/in-domain case2.mp4" type="video/mp4">
        </video> </td>
      </tr>
      </table>
      Our method follows the specified emotion and keeps the shape of face, while other methods fail in generating right lips, following the emotion, or keeping the shape of face.

    <p>&nbsp;&nbsp;&nbsp;</p>

    <h4 class="title is-5">Part2. In-The-Wild Comparison</h4>
    <table style="border-collapse:separate; border-spacing:0px 10px;">
      <tr>
        <td width="50%"> <video id="showcases3" controls="" playsinline="" height="90%">
          <source src="./assets/video/out-of-domain case1.mp4" type="video/mp4">
        </video> </td>
        <td width="50%"> <video id="showcases4" controls="" playsinline="" height="90%">
          <source src="./assets/video/out-of-domain case2.mp4" type="video/mp4">
        </video> </td>
      </tr>
      </table>
      Our method generates the most accurate lip when following the specific emotion.

    <br><br>
    <h3 class="title is-4">Quantitative Evaluation</h3>
    <div class="columns is-vcentered interpolation-panel">
      <div class="column has-text-centered">
        <img src="./assets/img/Quantitative_Evaluation.png">
        <p> Comparison with SOTAs on the MEAD dataset. The best results are bold and the second-best results are bracketed.</p>
      </div>
    </div>

    <br><br>
    <h3 class="title is-4">User Study</h3>
    <div class="columns is-vcentered interpolation-panel">
      <div class="column has-text-centered">
        <img src="./assets/img/user_study.png" width="55%">
        <p>
We conduct a user study with 30 users. We randomly select 10 speeches from the testing set: 5 for same-ID generation and the other 5 for cross-ID generation. Detailed setting is described in our supplement. We ask the users to separately identify the best anonymous models for each speech, according to lip synchronization, expression reality, and visual quality.</p>
      </div>
    </div>

  </div>
</section>

<!-- Ablation Studies -->
<section class="section">
  <div class="container is-max-desktop">

      <div class="section-title">
        <h2 class="title is-3">Ablation Studies</h2>
      </div>

    <h3 class="title is-4">Text-prompt guided emotional talking head generation</h3>
    <video controls style="width: 80%; margin: auto; display: block;">
      <source src="./assets/video/text.mp4" type="video/mp4">
    </video>

    <p>&nbsp;&nbsp;&nbsp;</p>

    <h3 class="title is-4">Audio-prompt guided emotional talking head generation</h3>
    <video controls style="width: 80%; margin: auto; display: block;">
      <source src="./assets/video/audio.mp4" type="video/mp4">
    </video>

  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
      <h1 class="title is-4">
          Ethical Consideration
      </h1>
      <div class="content has-text-justified-desktop">
          <p>

            We should use technology responsibly and be careful about the synthesized content. There is a risk that EasyTalking could be misused. Thus, we will restrict the access to our code and pre-trained models when making the code public, likely by validating the institutional email. Furthermore, we also consider embedding visible or invisible digital watermarks in any generated content.

          </p>

      </div>

      <h1 class="title is-4">
          Removal Policy
      </h1>
      <div class="content has-text-justified-desktop">

           <p>

         Please be aware that all videos on this page are algorithmically generated from publicly available sources and are intended solely for academic demonstrations and algorithm comparisons.  Any other form of usage is prohibited. Besides, if required by the original image owner or in the case of misuse of the models, the images, models, and codes associated with this project may be removed at any time.
          </p>
      </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website source code is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
